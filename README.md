# Applied Data Engineering Certificate Program

This repository contains notes, projects, and exercises from the Applied Data Engineering Certificate Program. The course is designed to equip students with the practical skills required for real-world data engineering roles. It covers a range of tools and technologies, from Linux and Docker to Big Data, Cloud, and Data Pipeline Automation.

## Course Modules

### 1. Linux and Docker
- **Focus**: Fundamentals of Linux, Docker containerization, automation, and cloud computing.
- **Skills**: Linux Commands, Shell Scripting, Docker, Docker Compose.
- **Tools**: Ubuntu, VSCode, VMBox, Docker.
- **Mini Projects**: Build and deploy applications using Docker containers.
    - [Project 1: Toronto Climate Data ETL Pipeline](https://github.com/sanyassyed/DataEngineering_Toronto_Climate_Data_ETL_Pipeline): This project extracts the climate data for the specified station id for the specified years, transforms it, concatenates it and loads it into a single csv file.
    - [Project 2: Data Processing Using Docker](https://github.com/sanyassyed/DataEngineering_Data_Processing_Using_Docker): This project creates a docker container for data processing

### 2. Python for Data Engineering
- **Focus**: Python for cloud-based data engineering tasks.
- **Skills**: Python, AWS EC2, S3, Lambda, Docker.
- **Tools**: Python, AWS S3, AWS EC2, Lambda, Serverless.
- **Project**: Python applications on cloud and serverless deployments.

### 3. Modern Data Stack
- **Focus**: Data warehouse technologies and SQL-based ELT transformation.
- **Skills**: ELT, Data Modeling, Snowflake, dbt, Airbyte.
- **Tools**: Snowflake, Redshift, DBT, Fivetran, Hightouch, Postgres.
- **Project**: Develop and orchestrate ELT pipelines with dbt.

### 4. Big Data and Data Lake
- **Focus**: Big Data processing using Spark and Data Lake architecture.
- **Skills**: PySpark, AWS EMR, Data Lake.
- **Tools**: Databricks, EMR, Spark, S3, Glue.
- **Project**: End-to-end Big Data processing with Spark and AWS Glue.

### 5. Build Data Pipelines
- **Focus**: Orchestration and automation of data pipelines.
- **Skills**: Apache Airflow, AWS Lambda, Step Functions.
- **Tools**: Airflow, Dagster, Prefect, Step Functions, Lambda.
- **Project**: Develop and deploy complex data pipelines.

### 6. NoSQL Databases
- **Focus**: Work with NoSQL databases like DynamoDB and Elasticsearch.
- **Skills**: CAP Theorem, NoSQL Data Modeling, ELK Stack.
- **Tools**: DynamoDB, Elasticsearch, Logstash, Kibana.
- **Project**: Ingest and analyze log data using Elasticsearch.

### 7. Data Lakehouse and Streaming
- **Focus**: Combining Data Lake and Warehouse with real-time data processing.
- **Skills**: CDC, Apache Kafka, Spark Streaming.
- **Tools**: Kafka, Hudi, Iceberg, Debezium, Flink, Spark.
- **Project**: Set up real-time data pipelines using Apache Kafka and Spark Streaming.

### 8. Career Preparation
- **Focus**: Job search strategies and interview preparation.
- **Skills**: Resume building, system design, mock interviews, networking.
- **Support**: 1:1 career mentoring, job referrals, and networking opportunities.
