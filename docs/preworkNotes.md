# Prework
The tasks performed in the course prework can be found in this section.

## Guideline and basics

### Data Engineering Foundation Concepts [Source](https://s3.amazonaws.com/weclouddata/data/data/data%20engineering%20concepts.pdf)
* Data
    * Structured - data with rows and cols that can fit in a table
    * Semi-Structured - data that required flattening to fit into a source system. This data does not fit neatly into a table Eg: Json, YML,
    * Unstructured - text, images, sounds, videos etc
* Data Operations:
    * **Data Integration** - Data Integration involves establishing links between operational and analytical services and data sources to enable secure, reliable access to data across multiple systems.
    * **Data Transformation** - Operational data usually needs to be transformed into suitable structure and format for analysis
    * **Data Consolidation** - Data consolidation is the process of combining data that has been extracted from multiple data sources into a consistent structure
* Common Languages:
    * SQL
    * Python
    * R
    * Java
    * Scala
* DE Concepts:
    * Operational Data 
        * Consists of transanctional Data 
        * Generated by apps etc
        * Stored in relational or non-relational DB
    * Analytical Data
        * Data that has been optimized for analysis and reporting
        * Often stored in a Data Warehouse
    * Streaming Data
        * Streaming data refers to perpetual sources of data that generate data values in real-time, often relating to specific events
        * Sources like IOT, social media feed are examples of streaming data source
    * Data Pipelines
        * Data pipelines are used to orchestrate activities that transfer and transform data
        * They allow DE's to perform repeated ETL which could be trigerred based on schedules or in response to an event
    * Data Lakes
        * Is a **storage repository** that holds large amounts of data in native, raw formats which could be structured, semi-structured or un-structured
        * They are designed or built for scaling
    * Data Warehouse
        * Is a **centralized repository** of integrated data from various different sources.
        * Both current and historical data is stored in relational tables with schema which is optimized for analytical queries
    * Apache Spark
        * Is a parallel processing framework that takes advantage of in-memory processing and distributed file storage.
        * It is an OSS (Open Source Software) tool for big data scenarios
    * Core Responsibilities of a DE ***(DIM or ETL or CI or DW or BigData)***
        1. Design, Implement, and Manage solutions that integrate operational and analytical data sources or
        1. Extract operational data from multiple systems, Transform it into appropriate structures for analytics, and Load it into an analytical data store (usually referred to as ETL solutions).
        1. Implement solutions that 
            + Capture real-time stream of data
            + Ingest them into analytical data systems, often combining the real-time data with other application data that is processed in batches.
        1. To design and implement relational data warehouses and manage regular data loads into the table
        1. Being proficient in Spark, using notebooks and other code artifacts to process data in the data lake and prepare it for modelling and analyasis


## Software Installation:
Additional softwares intalled include:
* Dbeaver
* Jupyter
* VMWare

## SQL
## Python
## Networking & Linux
## AWS
## Docker
## Microsoft Azure
## dbt
## Git
## Scala
